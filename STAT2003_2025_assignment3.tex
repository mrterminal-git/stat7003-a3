\documentclass[12pt]{article}
\input{preamble}

\begin{document}
\begin{center}

{\Large  {\bf Mathematical Probability (STAT2003/STAT7003)}

Assignment 3 - Semester 1, 2025.

}
\end{center}

The due date/time is given on Blackboard. STAT7003 students have additional items,\\ marked with a ``{\bf (*STAT7003)}''. The assignment has 5 questions with multiple items per question. Note that STAT7003 students (master's) need to complete all items. In contrast, {\bf STAT2003 students will only be marked on items not marked as STAT7003}. The weighting of the items is indicated in each question.\\ 


\begin{enumerate}

%%%%%%%%%%%%%%%%%%%%
% Q1
%%%%%%%%%%%%%%%%%%%%
\item Let $X_1 \sim \Nor(\mu_1, \sigma^2)$ and $X_2 \sim \Nor(\mu_2, \sigma^2)$ be independent random variables. Consider $Y_1 = X_1+X_2$ and $Y_2 = X_1-X_2$.   

\begin{enumerate}
\item Using moment generating function (MGF), show that $Y_1$ and $Y_2$ are independent. \\
\emph{Note:} You may use the fact that the joint MGF of independent random variables is the product of the individual MGFs. 
			\hfill [7 marks]
%
\\
\textbf{Answer:}
\\
The moment generating function (MGF) of a normal random variable $X \sim \Nor(\mu, \sigma^2)$ is given by:
\[
M_X(t) = \exp\left(\mu t + \frac{\sigma^2 t^2}{2}\right)
\]
Thus, the MGFs of $X_1$ and $X_2$ are:
\[
M_{X_1}(t) = \exp\left(\mu_1 t + \frac{\sigma^2 t^2}{2}\right)
\]
\[
M_{X_2}(t) = \exp\left(\mu_2 t + \frac{\sigma^2 t^2}{2}\right)
\]
The joint MGF of $X_1$ and $X_2$ is given by:
\[
M_{X_1, X_2}(t_1, t_2) = M_{X_1}(t_1) \cdot M_{X_2}(t_2) = \exp\left(\mu_1 t_1 + \frac{\sigma^2 t_1^2}{2}\right) \cdot \exp\left(\mu_2 t_2 + \frac{\sigma^2 t_2^2}{2}\right)
\]
Now, we can express $Y_1$ and $Y_2$ in terms of $X_1$ and $X_2$:
\[
Y_1 = X_1 + X_2
\]
\[
Y_2 = X_1 - X_2
\]
To find the joint MGF of $Y_1$ and $Y_2$, we need to express $X_1$ and $X_2$ in terms of $Y_1$ and $Y_2$. We can do this by solving the equations:
\[
X_1 = \frac{Y_1 + Y_2}{2}
\]
\[
X_2 = \frac{Y_1 - Y_2}{2}
\]
Substituting these expressions into the joint MGF of $X_1$ and $X_2$, we get:
\[
M_{Y_1, Y_2}(t_1, t_2) = M_{X_1, X_2}\left(\frac{t_1 + t_2}{2}, \frac{t_1 - t_2}{2}\right)
\]
Substituting the MGFs of $X_1$ and $X_2$, we have:
\[
M_{Y_1, Y_2}(t_1, t_2) = \exp\left(\mu_1 \frac{t_1 + t_2}{2} + \frac{\sigma^2}{2}\left(\frac{t_1 + t_2}{2}\right)^2\right) \cdot \exp\left(\mu_2 \frac{t_1 - t_2}{2} + \frac{\sigma^2}{2}\left(\frac{t_1 - t_2}{2}\right)^2\right)
\]
Now, we can simplify this expression:
\[
M_{Y_1, Y_2}(t_1, t_2) = \exp\left(\frac{\mu_1 + \mu_2}{2} t_1 + \frac{\mu_1 - \mu_2}{2} t_2 + \frac{\sigma^2}{8}\left(t_1^2 + t_2^2 + 2t_1 t_2\right) + \frac{\sigma^2}{8}\left(t_1^2 - t_2^2 - 2t_1 t_2\right)\right)
\]
This simplifies to:
\[
\boxed {M_{Y_1, Y_2}(t_1, t_2) = \exp\left(\frac{\mu_1 + \mu_2}{2} t_1 + \frac{\mu_1 - \mu_2}{2} t_2 + \frac{\sigma^2}{4} t_1^2 + \frac{\sigma^2}{4} t_2^2\right)}
\]
This is the joint MGF of $Y_1$ and $Y_2$. This can also be expressed as the product of the individual MGFs:
\[
M_{Y_1, Y_2}(t_1, t_2) = \exp\left(\frac{\mu_1 + \mu_2}{2} t_1 + \frac{\sigma^2}{4} t_1^2 \right) \cdot \exp\left(\frac{\mu_1 - \mu_2}{2} t_2 + \frac{\sigma^2}{4} t_2^2\right)
\]
\[
= M_{Y_1}(t_1) \cdot M_{Y_2}(t_2)
\]

Since the joint MGF can be expressed as a product of the individual MGFs, we conclude that $Y_1$ and $Y_2$ are independent random variables.


\item Find the joint probability density function (PDF) of $Y_1$ and $Y_2$. 
			\hfill [3 marks]
%
\\
\textbf{Answer:}
\\

The joint PDF of $X_1$ and $X_2$ is given by the product of their individual PDFs:
\begin{align*}
f_{X_1, X_2}(x_1, x_2) &= f_{X_1}(x_1) \cdot f_{X_2}(x_2) \\
&= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x_1 - \mu_1)^2}{2\sigma^2}\right) \cdot \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x_2 - \mu_2)^2}{2\sigma^2}\right) \\
&= \frac{1}{2\pi \sigma^2} \exp\left(-\frac{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}{2\sigma^2}\right)
\end{align*}

The transformation from $(X_1, X_2)$ to $(Y_1, Y_2)$ is given by the equation:
\begin{align*}
f_{Y_1, Y_2}(y_1, y_2) &= f_{X_1, X_2}\left(x_1,x_2\right) \cdot \left|J\right| \\
\end{align*}

The Jacobian of the transformation is given by:
\begin{align*}
J &= \begin{vmatrix}
\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\
\frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2}
\end{vmatrix} \\
&= \begin{vmatrix}
\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2}
\end{vmatrix} \\
&= \left(\frac{1}{2}\right)\left(-\frac{1}{2}\right) - \left(\frac{1}{2}\right)\left(\frac{1}{2}\right) \\
&= -\frac{1}{4} - \frac{1}{4} \\
&= -\frac{1}{2}
\end{align*}

Thus, the absolute value of the Jacobian is $|J| = \frac{1}{2}$.
Now, we can express the joint PDF of $Y_1$ and $Y_2$ as:

\begin{align*}
f_{Y_1, Y_2}(y_1, y_2) &= f_{X_1, X_2}\left(\frac{y_1 + y_2}{2}, \frac{y_1 - y_2}{2}\right) \cdot \left|J\right| \\
&= \frac{1}{2\pi \sigma^2} \exp\left(-\frac{\left(\frac{y_1 + y_2}{2} - \mu_1\right)^2 + \left(\frac{y_1 - y_2}{2} - \mu_2\right)^2}{2\sigma^2}\right) \cdot \frac{1}{2} \\
\end{align*}

This simplifies to:
\[
\boxed{f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{4\pi \sigma^2} \exp\left(-\frac{\left(\frac{y_1 + y_2}{2} - \mu_1\right)^2 + \left(\frac{y_1 - y_2}{2} - \mu_2\right)^2}{2\sigma^2}\right)}
\]

This is the joint PDF of $Y_1$ and $Y_2$, which is a bivariate normal distribution with means $\mu_1$ and $\mu_2$, and variance $\sigma^2$.
%

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\vspace{5pt}
% Q2
%%%%%%%%%%%%%%%%%%%%
\item 
Two machines, A and B, participate in a performance test where each is timed to complete a specific task. The time it takes a machine to complete the task follows a geometric distribution with success probability $p$ per minute; that is, in each minute, there is a probability $p$ that the machine completes the task. Please use the parameterisation of the geometric distribution given in the course notes.  

Let $A$ and $B$ denote the number of minutes taken by machines A and B, respectively, to complete the task. Consider
\begin{itemize}
\item $X=\min(A, B)$: the earliest time the task is completed by either machine,
\item $Y = A-B$: the difference in completion times, and 
\item $Z = \frac{A}{A+B}$: the proportion of total time taken by machine A.
\end{itemize}
 
\begin{enumerate}
\item Find the joint probability mass function (PMF) of $X$ and $Y$. 
			\hfill [8 marks]
%
\\
\textbf{Answer:}
\\
The probability disibutions of $A$ and $B$ are given by:
\begin{align*}
P(A=a) &= (1-p)^{a-1}p \quad \text{for } k=1,2,\ldots \\
P(B=b) &= (1-p)^{b-1}p \quad \text{for } k=1,2,\ldots
\end{align*}

We break the problem into two cases - when $a\geq b$ and when $a<b$.

\subsubsection*{Case 1: $a\geq b$, $y \geq 0$}

In this case, we have $X = b$ and $Y = a-b$. The joint PMF is given by:
\begin{align*}
P(X=x, Y=y) &= P(A = x + y, B=x) \\
&= P(A = x+y) \cdot P(B=x) \\
&= (1-p)^{(x+y)-1}p \cdot (1-p)^{x-1}p \\
&= \boxed{p^2(1-p)^{2x+y-2}} \\
\end{align*}

\subsubsection*{Case 2: $a<b$, $y < 0$}
In this case, we have $X = a$ and $Y = a-b$. The joint PMF is given by:
\begin{align*}
P(X=x, Y=y) &= P(A = x, B=x-y) \\
&= P(A = x) \cdot P(B=x-y) \\
&= (1-p)^{x-1}p \cdot (1-p)^{(x-y)-1}p \\
&= \boxed{p^2(1-p)^{2x-y-2}} \\
\end{align*}

Therefore, the joint PMF of $X$ and $Y$ is given by:
\begin{align*}
P(X=x, Y=y) &= \begin{cases}
p^2(1-p)^{2x+y-2} & \text{if } y \geq 0 \\
p^2(1-p)^{2x-y-2} & \text{if } y < 0
\end{cases}
\end{align*}


\item Are $X$ and $Y$ independent? Justify your answer.  
			\hfill [2 marks]
%
\\
\textbf{Answer:}
\\
To check if $X$ and $Y$ are independent, we need to see if the joint PMF can be expressed as the product of the marginal PMFs.
The marginal PMF of $X$ can be found by summing the joint PMF over all possible values of $Y$:

\begin{align*}
P(X=x) &= \sum_{y=-\infty}^{\infty} P(X=x, Y=y) \\
&= \sum_{y=0}^{\infty} p^2(1-p)^{2x+y-2} + \sum_{y=-\infty}^{-1} p^2(1-p)^{2x-y-2} \\
&= p^2(1-p)^{2x-2} \sum_{y=0}^{\infty} (1-p)^{y} + p^2(1-p)^{2x-2} \sum_{y=-\infty}^{-1} (1-p)^{-y} \\
\end{align*}

The left sum is a geometric series with the first term $1$ and common ratio $(1-p)$, which converges:

\begin{align*}
\sum_{y=0}^{\infty} (1-p)^{y} &= \frac{1}{p} \\
\end{align*}

The right sum can be rexpressed as:

\begin{align*}
\sum_{y=-\infty}^{-1} (1-p)^{-y} &= \sum_{k=1}^{\infty} (1-p)^{k} \\
&= \frac{(1-p)}{p} \\
\end{align*}

Subsituting these results back into the marginal PMF of $X$, we have:
\begin{align*}
P(X=x) &= p^2(1-p)^{2x-2} \left(\frac{1}{p} + \frac{1-p}{p}\right) \\
&= p(1-p)^{2x-2} \left( 2-p \right) \\
\end{align*}

The marginal PMF of $Y$ can be found similarly by summing the join PMF over all possible values of $X$:

\begin{align*}
P(Y=y) &= \sum_{x=-\infty}^{\infty} P(X=x, Y=y) \\
&= \begin{cases}
\sum_{x=0}^{\infty} p^2(1-p)^{2x+y-2} & \text{if } y \geq 0 \\
\sum_{x=0}^{\infty} p^2(1-p)^{2x-y-2} & \text{if } y < 0
\end{cases}	\\
&= \begin{cases}
	p^2(1-p)^{y-2} \sum_{x=0}^{\infty} (1-p)^{2x} & \text{if } y \geq 0 \\
	p^2(1-p)^{-y-2} \sum_{x=0}^{\infty} (1-p)^{2x} & \text{if } y < 0
\end{cases} \\
\end{align*}

The sum term is a geometric series with the first term $1$ and common ratio $(1-p)^2$, which converges:

\begin{align*}
\sum_{x=0}^{\infty} (1-p)^{2x} &= \frac{1}{p(1-p)} \\
\end{align*}

Substituting this result back into the marginal PMF of $Y$, we have:
\begin{align*}
P(Y=y) &= \begin{cases}
p(1-p)^{y-2} \left(\frac{1}{p(1-p)}\right) & \text{if } y \geq 0 \\
p(1-p)^{-y-2} \left(\frac{1}{p(1-p)}\right) & \text{if } y < 0
\end{cases} \\
&= \begin{cases}
(1-p)^{y-3} & \text{if } y \geq 0 \\
(1-p)^{-y-3} & \text{if } y < 0
\end{cases}
\end{align*}

Now, we can check if the joint PMF can be expressed as the product of the marginal PMFs:

\begin{align*}
P(X=x, Y=y) &= \begin{cases}
(1-p)^{y-3}p(1-p)^{2x-2} \left( 2-p \right) & \text{if } y \geq 0 \\
(1-p)^{-y-3}p(1-p)^{2x-2} \left( 2-p \right) & \text{if } y < 0
\end{cases} \\
\end{align*}

By inspection, we can see that the joint PMF cannot be expressed as the product of the marginal PMFs. Therefore, $X$ and $Y$ are not independent.
%

\item Determine the joint PMF of $A$ and $A+B$. 
			\hfill [4 marks]
%
\\
\textbf{Answer:}
\\
Let $M = A + B$. The joint PMF of $A$ and $M$ can be found by the following:

\begin{align*}
P(A=a, M=m) &= P(A=a, A+B=m) \\
&= P(A=a, B=m-a) \\
&= P(A=a) \cdot P(B=m-a) \\
&= (1-p)^{a-1}p \cdot (1-p)^{(m-a)-1}p \\
&= p^2(1-p)^{m-2} \\
\end{align*}
%

\item Find the PMF of $Z$.  
			\hfill [6 marks]
%
\\
\textbf{Answer:}
\\
The PMF of $Z$ can be found by the following:
\begin{align*}
P(Z=z) &= P\left(\frac{A}{A+B} = z\right) \\
&= P(A = z(A+B)) \\
&= P(A = zM) \\
&= P(A = zm, M=m) \\
&= \boxed{p^2(1-p)^{m-2}} \\
\end{align*}
%


\item{\bf (*STAT7003)} 
Suppose the test is repeated independently $N$ times. Let $N_A$ be the number of times machine A completes the task before machine B. What is the distribution of $N_A$? What is the probability that machine A is faster than machine B in more than half of the $N$ trials?
			\\\phantom{1}\hfill [10 marks]
%
\\
\textbf{Answer:}
\\
The number of times machine A completes the task before machine B in $N$ trials is given by the expression:

\begin{align*}
N_A &= \sum_{i=1}^{N} I_i \\
I_i &= \begin{cases}
1 & \text{if machine A is faster than machine B in trial } i \\
0 & \text{otherwise}
\end{cases}
\end{align*}

Therefore, $N_A$ is the sum of $N$ independent Bernoulli random variables, where each $I_i$ has a success probability of $p$.
The distribution of $N_A$ is given by the binomial distribution with parameters $N$ and $p$:

\begin{align*}
N_A \sim \mathrm{Binomial}(N, p) \\
\end{align*}


\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\vspace{5pt}
% Q3
%%%%%%%%%%%%%%%%%%%%
\item An engineer needs to simplify the processing of sensor data by rounding the recorded values to the nearest whole number. For example, both sensor readings of 9.53 and 10.47 will be rounded to 10. To model the rounding errors, the engineer assumes that the rounding errors are independent and follow a uniform distribution between -0.5 and 0.5. Let $X_1, X_2, \ldots, X_n$ represent the rounding errors for $n$ measurements.  

\begin{enumerate}
\item Find the expected value and variance of each rounding error $X_i$.
			\hfill [2 marks]
%
\\
\textbf{Answer:}
\\
The expected value of a uniform distribution $X \sim U(a, b)$ is given by:

\item Using Chebyshev's inequality, calculate an upper bound for the probability that the total rounding error exceeds 10\% of the number of measurements, that is, $\Pm\left(|\sum_{i=1}^n X_i| > 0.1 n\right)$. 
			\\\phantom{1}\hfill [2 marks]
%
\item A researcher is interested in how the mean absolute error (MAE) $\frac{1}{n} \sum_{i=1}^n |X_i|$ behaves as the number of measurements $n$ becomes large. What can you conclude using the law of large numbers?
			\hfill [4 marks]
%
\item For large $n$, find the distribution of the MAE using the central limit theorem. 
			\hfill [2 marks]
%
\item 
{\bf (*STAT7003)} Repeat (d) with MAE replaced by the mean square error (MSE) $\frac{1}{n} \sum_{i=1}^n X_i^2$. 
			\\\phantom{1}\hfill [5 marks]
%
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\vspace{5pt}
% Q4
%%%%%%%%%%%%%%%%%%%%
\item A sports equipment store sells limited edition tennis rackets. Due to the exclusive nature of the product and limited display space, the store can hold a maximum of four rackets at any one time. Rackets must be stored individually to avoid damage, and storing them elsewhere in the shop is not permitted.
%
Each day, the number of rackets sold (provided there is sufficient stock) follows the same distribution:
\begin{itemize}
	\item There is a 40\% chance that no rackets are sold.
	\item There is a 40\% chance that one racket is sold.
	\item There is a 20\% chance that two rackets are sold.
	\end{itemize}
%
If the store has no rackets in stock at the end of the day, the manager places a restocking order for four new rackets, which are delivered before the store opens the following morning. Each delivery incurs a fixed cost of $C$.  
%
\begin{enumerate}
\item Give the one-step transition matrix for the number of rackets in stock at the start of each day, given the stock level at the start of the previous day. Draw the corresponding transition graph. 
			\\\phantom{1}\hfill [6 marks]
%
\item Find the limiting distribution for the number of rackets in stock when the shop opens, using your transition matrix in (a). 
			\hfill [5 marks]
%
\item Determine the expected long-run average number of restocking orders placed per day. 
			\\\phantom{1}\hfill [2 marks]
%
\item If a customer arrives and no rackets are in stock, the sale is lost. Calculate the expected number of lost sales per day in the long run. 
			\hfill [2 marks]
%
\item Each successful sale generates a profit of $P$, and the delivery charge remains as $C$ regardless of the quantity ordered. Use {\sf Python} to simulate the sales and inventory of this store. Implement two restocking strategies: (i) restock when there are no rackets left, and (ii) restock if there are fewer than two rackets at the end of the day. 
Start with an initial stock of four rackets. Set $P=10$ and $C=15$. Simulate $1000$ days and report the average number of restocking orders, the average number of sales lost, and the average profit from sales. Compare the two restocking strategies based on the simulation results.   
			\hfill [15 marks]
%
\item {\bf (*STAT7003)} To determine the most profitable strategy, derive a condition (expressed in terms of $C$ and $P$) under which the manager should switch from strategy (i) to strategy (ii).   
			\\\phantom{1}\hfill [5 marks] 
%
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\vspace{5pt}
\newpage
% Q5
%%%%%%%%%%%%%%%%%%%%
\item A smart irrigation system manages water delivery using the following components:
\begin{itemize}
\item two soil moisture sensors (Sensor A and Sensor B) monitors moisture levels. 
\item a central decision unit processes sensor data and determines when irrigation is needed. 
\item two electronic irrigation valves (Valve A and Valve B) which open if instructed. 
\item a power controller provides energy to all components.
\end{itemize}
The system is considered operational if both the power controller and the central decision unit are working, \textbf{and} at least one sensor is working, \textbf{and} at least one valve is working. 
%
\begin{enumerate}
\item Draw a block diagram that represents the system's components and their relationships. 
			\\\phantom{1}\hfill [4 marks] 
%
\item Give the structure function for this system.
			\hfill [2 marks] 
%
\item Assume all six components fail independently and have exponentially distributed lifetimes with mean $\lambda$ years. What is the probability that the system is working at some time $t>0$? 
			\\\phantom{1}\hfill [4 marks] 
%
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\end{document}
